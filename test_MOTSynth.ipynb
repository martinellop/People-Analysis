{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOTSynth_3 tester\n",
    "Here we try to take a frame from a MOTSynth video, applying data visualization (bounding boxes, instances, class).\n",
    "Since it is no clear how the coordinates are annotated (real world? or relative respect camera?) let's try to plot instance's coordinates in a 3d graph,\n",
    "to see how they are referenced. \n",
    "\n",
    "NOTE: MOT webpage says coordinates are in world coordinates, but on annotations we don't have camera coordinates/rotation :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from ReID.common.MOTutils import MOTannotation, plot_one_box\n",
    "\n",
    "from PeopleDetector.utils.datasets import LoadStreams, LoadImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\".\", \"inputs\", \"videos\", \"512\")\n",
    "video_path = os.path.join(data_folder, \"512.mp4\")\n",
    "annotations_path = os.path.join(data_folder, \"gt\", \"gt.txt\")\n",
    "target_frame = 3\n",
    "\n",
    "data = MOTannotation(annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (1/1800) d:\\Data\\University\\People-Analysis\\inputs\\videos\\512\\512.mp4: \n",
      "video 1/1 (2/1800) d:\\Data\\University\\People-Analysis\\inputs\\videos\\512\\512.mp4: \n",
      "video 1/1 (3/1800) d:\\Data\\University\\People-Analysis\\inputs\\videos\\512\\512.mp4: \n",
      "(1080, 1920, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ea0d416bc0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#let's show the frame with all bounding box\n",
    "#matplotlib.use('Agg',force=True)\n",
    "video = LoadImages(video_path,1920)\n",
    "\n",
    "frame_counter = 0\n",
    "for path, img, im0s, vid_cap in video:\n",
    "    frame_counter+=1\n",
    "    if(frame_counter < target_frame):\n",
    "        continue\n",
    "    \n",
    "    for el in data[target_frame]:\n",
    "        xyxy = el.bb_left_, el.bb_top_, el.bb_left_ + el.bb_width_, el.bb_top_ + el.bb_height_\n",
    "        #print(xyxy)\n",
    "        plot_one_box(xyxy, im0s, label=str(el.id_), line_thickness=4)\n",
    "    break\n",
    "print(im0s.shape)\n",
    "\n",
    "gbr = im0s#[...,[2,1,0]]#.copy()\n",
    "#cv2.imshow(\"test\", gbr)\n",
    "#cv2.waitKey()\n",
    "plt.imshow(gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#let's show the 3d position of people in first frame.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for el in data[target_frame]:\n",
    "    ax.scatter(el.x_, el.y_, el.z_)\n",
    "    ax.text(el.x_,el.y_,el.z_,  '%s' % (str(el.id_)), size=10, zorder=1,  color='k')\n",
    "\n",
    "\n",
    "ax.scatter(0,0,0 , marker='^')\n",
    "ax.text(0,0,0,  '_O_', size=10, zorder=1,  color='k')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvcs-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6e259434f4333afa1cc5dea76fb96711b9c36ae1cda3e11247821c069fa262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
