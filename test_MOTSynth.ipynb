{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOTSynth_3 tester\n",
    "Here we try to take a frame from a MOTSynth video, applying data visualization (bounding boxes, instances, class).\n",
    "Since it is no clear how the coordinates are annotated (real world? or relative respect camera?) let's try to plot instance's coordinates in a 3d graph,\n",
    "to see how they are referenced. \n",
    "\n",
    "NOTE: MOT webpage says coordinates are in world coordinates, but on annotations we don't have camera coordinates/rotation :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tkinter import *\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg',force=True)\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from PeopleDetector.utils.plots import plot_one_box\n",
    "from PeopleDetector.utils.datasets import LoadStreams, LoadImages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MOTline:\n",
    "    def __init__(self, frame:int, id:int,bb_left:int, bb_top:int, bb_width:int, bb_height:int, \n",
    "    visibility:float, class_:int, confidence:float, x:float, y:float, z:float):\n",
    "        self.frame_ = frame\n",
    "        self.id_ = id\n",
    "        self.bb_left_ = bb_left\n",
    "        self.bb_top_ = bb_top\n",
    "        self.bb_width_ = bb_width\n",
    "        self.bb_height_ = bb_height\n",
    "        self.visibility = visibility,\n",
    "        self.class_ = class_\n",
    "        self.confidence_ = confidence\n",
    "        self.x_ = x\n",
    "        self.y_ = y\n",
    "        self.z_ = z\n",
    "\n",
    "class MOTannotation:\n",
    "    def __init__(self, path:str):\n",
    "        annotations = open(path, 'r')\n",
    "        self.data_ = {}\n",
    "        for line in annotations:\n",
    "            l = line.strip().split(',')\n",
    "            v = MOTline(int(l[0]),int(l[1]),int(l[2]),int(l[3]),int(l[4]),int(l[5]),float(l[6]),int(l[7]),float(l[8]),float(l[9]),float(l[10]),float(l[11]))\n",
    "            if not v.frame_ in self.data_.keys():\n",
    "                self.data_[v.frame_] = []\n",
    "            self.data_[v.frame_].append(v)\n",
    "        annotations.close()\n",
    "\n",
    "    def __getitem__(self, frame:int):\n",
    "        return self.data_[frame]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(\".\", \"inputs\", \"videos\", \"512\")\n",
    "video_path = os.path.join(data_folder, \"512.mp4\")\n",
    "annotations_path = os.path.join(data_folder, \"gt\", \"gt.txt\")\n",
    "target_frame = 3\n",
    "\n",
    "data = MOTannotation(annotations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (1/1800) d:\\Data\\University\\People-Analysis\\inputs\\videos\\512\\512.mp4: \n",
      "video 1/1 (2/1800) d:\\Data\\University\\People-Analysis\\inputs\\videos\\512\\512.mp4: \n",
      "video 1/1 (3/1800) d:\\Data\\University\\People-Analysis\\inputs\\videos\\512\\512.mp4: \n",
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "#let's show the frame with all bounding box\n",
    "#matplotlib.use('Agg',force=True)\n",
    "video = LoadImages(video_path,1920)\n",
    "\n",
    "frame_counter = 0\n",
    "for path, img, im0s, vid_cap in video:\n",
    "    frame_counter+=1\n",
    "    if(frame_counter < target_frame):\n",
    "        continue\n",
    "    \n",
    "    for el in data[target_frame]:\n",
    "        xyxy = el.bb_left_, el.bb_top_, el.bb_left_ + el.bb_width_, el.bb_top_ + el.bb_height_\n",
    "        #print(xyxy)\n",
    "        plot_one_box(xyxy, im0s, label=str(el.id_), line_thickness=4)\n",
    "    break\n",
    "print(im0s.shape)\n",
    "\n",
    "gbr = im0s[...,[2,1,0]]#.copy()\n",
    "plt.imshow(gbr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's show the 3d position of people in first frame.\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "for el in data[target_frame]:\n",
    "    ax.scatter(el.x_, el.y_, el.z_)\n",
    "    ax.text(el.x_,el.y_,el.z_,  '%s' % (str(el.id_)), size=10, zorder=1,  color='k')\n",
    "\n",
    "\n",
    "ax.scatter(0,0,0 , marker='^')\n",
    "ax.text(0,0,0,  '_O_', size=10, zorder=1,  color='k')\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvcs-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6e259434f4333afa1cc5dea76fb96711b9c36ae1cda3e11247821c069fa262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
