{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(weights=['yolov7-tiny.pt'], source='./inputs/videos/example_MOT20.mp4', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=[0], agnostic_nms=False, augment=False, update=False, output='./outputs/videos', exist_ok=True, no_trace=False, save_only_crop=True, video_crop_min_area=7000, video_crop_max_area=-1, video_crop_min_frame_interval=30)\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "video 1/1 (1/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "19 persons, Done. (217.2ms) Inference, (15.0ms) NMS\n",
      "video 1/1 (2/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (3/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (4/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (5/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (6/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (7/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (8/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (9/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (10/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (11/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (12/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (13/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (14/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (15/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (16/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (17/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (18/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (19/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (20/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (21/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (22/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (23/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (24/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (25/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (26/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (27/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (28/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (29/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (30/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (31/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "20 persons, Done. (139.1ms) Inference, (2.0ms) NMS\n",
      "video 1/1 (32/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (33/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (34/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (35/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (36/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (37/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (38/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (39/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (40/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (41/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (42/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (43/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (44/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (45/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (46/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (47/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (48/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (49/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (50/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (51/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (52/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (53/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (54/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (55/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (56/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (57/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (58/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (59/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (60/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (61/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "23 persons, Done. (147.0ms) Inference, (1.0ms) NMS\n",
      "video 1/1 (62/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (63/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (64/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (65/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (66/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (67/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (68/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (69/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (70/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (71/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (72/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (73/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (74/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (75/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (76/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (77/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (78/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (79/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (80/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (81/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (82/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (83/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (84/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (85/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (86/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (87/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (88/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (89/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (90/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (91/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "19 persons, Done. (126.7ms) Inference, (2.0ms) NMS\n",
      "video 1/1 (92/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (93/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (94/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (95/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (96/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (97/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (98/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (99/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (100/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (101/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (102/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (103/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (104/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (105/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (106/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (107/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (108/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (109/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (110/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (111/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (112/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (113/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (114/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (115/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (116/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (117/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (118/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (119/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (120/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (121/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "22 persons, Done. (124.2ms) Inference, (2.0ms) NMS\n",
      "video 1/1 (122/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (123/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (124/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (125/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (126/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (127/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (128/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (129/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (130/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (131/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (132/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (133/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (134/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (135/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (136/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (137/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (138/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (139/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (140/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (141/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (142/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (143/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (144/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (145/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (146/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (147/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "video 1/1 (148/148) d:\\Data\\University\\People-Analysis\\inputs\\videos\\example_MOT20.mp4: \n",
      "Done. (2.323s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  0393065 torch 1.13.0+cpu CPU\n",
      "\n",
      "Model Summary: 200 layers, 6219709 parameters, 229245 gradients\n",
      "c:\\Users\\pietr\\.conda\\envs\\cvcs-project\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Example of video processing (example videos in /inputs/videos/)\n",
    "# --> we also specified to save only boxes with a minum of 15000 pixels (we don't want too small boxes)\n",
    "# --> and to save with a minum interval of 30 frames (=1s), in order to have a minimum ofdiversity in results.\n",
    "\n",
    "# add '--exist-ok' if output override is okay for you\n",
    "# add '--save_only_crop' if in output you want only crops\n",
    "# add '--class' and a class number for only that class outputs --> 'person' is class 0\n",
    "# add  '--video_crop_min_area' and a number for setting the minimum number of pixels in detected box for a crop\n",
    "# add  '--video_crop_min_frame_interval' and a number for setting the minimum number of frames to be skipped from last crop-saved frame\n",
    "\n",
    "!python PeopleDetector/peopleDetector.py --source ./inputs/videos/example_MOT20.mp4 --output ./outputs/videos --weights yolov7-tiny.pt --class 0 --exist-ok --save_only_crop --video_crop_min_area 15000 --video_crop_min_frame_interval 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utils is imported.\n",
      "Namespace(weights=['yolov7-tiny.pt'], source='./inputs/images/*.jpg', img_size=640, conf_thres=0.25, iou_thres=0.45, device='', view_img=False, save_txt=False, save_conf=False, nosave=False, classes=[0], agnostic_nms=False, augment=False, update=False, output='./outputs/images', exist_ok=True, no_trace=False, save_only_crop=True, video_crop_min_area=0, video_crop_min_frame_interval=1)\n",
      "Fusing layers... \n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n",
      "4 persons, Done. (347.3ms) Inference, (5.0ms) NMS\n",
      "Done. (271.5ms) Inference, (3.0ms) NMS\n",
      "2 persons, Done. (267.7ms) Inference, (2.0ms) NMS\n",
      "2 persons, Done. (255.8ms) Inference, (1.5ms) NMS\n",
      "Done. (236.6ms) Inference, (2.1ms) NMS\n",
      "2 persons, Done. (198.0ms) Inference, (3.0ms) NMS\n",
      "Done. (1.735s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  6fe83be torch 1.13.0+cpu CPU\n",
      "\n",
      "Model Summary: 200 layers, 6219709 parameters, 229245 gradients\n",
      "c:\\Users\\pietr\\.conda\\envs\\cvcs-project\\lib\\site-packages\\torch\\functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\\aten\\src\\ATen\\native\\TensorShape.cpp:3191.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "# Example of images processing (example images in /inputs/images/)\n",
    "\n",
    "# add '--exist-ok' if output override is okay for you\n",
    "# add '--save_only_crop' if in output you want only crops\n",
    "# add '--class' and a class number for only that class outputs --> 'person' is class 0\n",
    "\n",
    "!python PeopleDetector/peopleDetector.py --source ./inputs/images/*.jpg --output ./outputs/images --weights yolov7-tiny.pt --exist-ok --class 0 --save_only_crop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('cvcs-project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b6e259434f4333afa1cc5dea76fb96711b9c36ae1cda3e11247821c069fa262"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
